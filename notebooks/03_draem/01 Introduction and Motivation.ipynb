{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0290dd132ba4317",
   "metadata": {},
   "source": [
    "# **Project: Anomaly Detection for AITEX Dataset**\n",
    "#### Track: DR√ÜM\n",
    "## `Notebook 1`: Transitioning to DR√ÜM for Visual Anomaly Detection\n",
    "**Author**: Oliver Grau \n",
    "\n",
    "**Date**: 27.03.2025  \n",
    "**Version**: 1.0\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "- [1. Summary of Earlier Approaches](#1-summary-of-earlier-approaches)\n",
    "- [2. Observations on the AITEX Dataset](#2-observations-on-the-aitex-dataset)\n",
    "- [3. Enter DR√ÜM: A Different Paradigm](#3-enter-draem--a-different-paradigm)\n",
    "- [4. What you should already know](#4-what-you-should-already-know)\n",
    "- [5. Roadmap of Upcoming Notebooks](#5-roadmap-of-upcoming-notebooks)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Summary of Earlier Approaches\n",
    "\n",
    "### 1. **Variational Autoencoder (VAE)**\n",
    "- **Goal**: Learn to reconstruct normal patches and detect anomalies via high reconstruction error.\n",
    "- **Key properties**:\n",
    "  - Encoder compresses input into latent space.\n",
    "  - Decoder reconstructs image from latent.\n",
    "  - Anomalies detected using pixel-wise L1/MSE loss.\n",
    "\n",
    "**üí° Challenges with VAE**:\n",
    "- Reconstructions tend to be **blurry**.\n",
    "- **Small/texture anomalies** are often **missed**.\n",
    "- Struggles with **high-frequency detail** ‚Äî common in textiles.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **PatchCore (with ResNet, ConvNeXt, DINOv2, DenseNet and Custom FFT and ShallowCNN)**\n",
    "- **Goal**: Compare patch-level CNN features from a test image to a memory bank of normal patches.\n",
    "- **Pipeline**:\n",
    "  - Extract intermediate CNN features (from ResNet or ViT).\n",
    "  - Flatten spatial features into patch embeddings.\n",
    "  - Store them in a memory bank (index).\n",
    "  - Use **FAISS** to perform **nearest-neighbor search**.\n",
    "  - Use the L2 distance to the closest normal patch as anomaly score.\n",
    "\n",
    "**‚úÖ Pros**:\n",
    "- Does not require training.\n",
    "- Works well on texture-rich industrial datasets.\n",
    "\n",
    "**‚ö† Limitations Observed**:\n",
    "- **FAISS is heavy** even on GPU.\n",
    "- Difficult to scale.\n",
    "- Requires **a lot of memory**.\n",
    "- **Recall maxed out around 37%** ‚Äî anomalies often missed.\n",
    "- Results vary heavily depending on layer and backbone.\n",
    "- Patch-based decision can miss **global coherence**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Observations on the AITEX Dataset\n",
    "\n",
    "- High-resolution textile images (4096√ó256), patched to 256√ó256.\n",
    "- **Defects are small** and can be subtle (e.g., loose threads, weave faults).\n",
    "- **Noisy background** from machinery or conveyor adds complexity.\n",
    "- **Histogram equalization and various backbones** (ResNet50, ConvNeXt, DINOv2) had **limited effect**.\n",
    "- Precision was sometimes good (e.g., 80%), but **recall remained low (~35%)** ‚Äî meaning most defects were missed.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Enter DR√ÜM: A Different Paradigm\n",
    "\n",
    "> **DR√ÜM: Denoising Autoencoder with Realistic Anomalies**\n",
    "\n",
    "Instead of:\n",
    "- memorizing what normal looks like (VAE),\n",
    "- or matching features to a memory bank (PatchCore),\n",
    "\n",
    "üëâ **DR√ÜM learns to *reconstruct clean images from synthetically corrupted ones***.\n",
    "\n",
    "### üîß Basic Concepts\n",
    "\n",
    "#### 1. **Synthetic Anomalies**\n",
    "- During training, DR√ÜM adds **fake anomalies** (noise blobs, cut-paste artifacts) to normal images.\n",
    "- It learns to **reconstruct clean versions** and localize the anomalous regions.\n",
    "\n",
    "#### 2. **Dual Architecture**\n",
    "- **Reconstruction Network**: U-Net learns to clean corrupted input.\n",
    "- **Discriminator Network**: Learns to predict **pixel-wise anomaly masks**.\n",
    "\n",
    "#### 3. **Loss Functions**\n",
    "\n",
    "The model is trained with two separate loss branches:\n",
    "\n",
    "- **Reconstruction loss** ‚Äî encourages accurate reconstruction of the original (unmodified) input image.\n",
    "- **Segmentation loss** ‚Äî trains the model to segment anomalies using synthetic ground truth masks (i.e., masks used during training to indicate the corrupted region).\n",
    "\n",
    "---\n",
    "\n",
    "### **Reconstruction Losses**\n",
    "\n",
    "These are applied to the **autoencoder part** of the model, which learns to reconstruct the uncorrupted input from its corrupted version.\n",
    "\n",
    "- `L1_loss`:  \n",
    "  Measures the absolute difference between the reconstructed image and the original input. Promotes pixel-level similarity but ignores perceptual differences.\n",
    "\n",
    "- `SSIM_loss`:  \n",
    "  Structural Similarity Index. Encourages perceptual similarity by comparing local contrast, luminance, and structure. Helps produce sharper, more visually aligned reconstructions.\n",
    "\n",
    "- `FFT_loss`:  \n",
    "  Compares the log-magnitude of the FFT spectra of the original and reconstructed images. Useful for detecting brightness shifts and texture inconsistencies that may be invisible in pixel space.\n",
    "\n",
    "‚úÖ **Combined Reconstruction Loss** (example):  \n",
    "```python\n",
    "recon_loss = 0.7 * MSE_SSIM_Loss + 0.3 * FFT_Loss\n",
    "```\n",
    "Where `MSE_SSIM_Loss = 0.8 * L1_loss + 0.2 * SSIM_loss`\n",
    "\n",
    "This combined loss ensures that both fine details and global brightness/texture patterns are preserved in the reconstruction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Segmentation Losses**\n",
    "\n",
    "These are used to train the **anomaly segmentation head**, which learns to predict the anomaly mask based on residuals and deep features.\n",
    "\n",
    "- `BCE_loss`:  \n",
    "  Binary Cross-Entropy loss between the predicted anomaly map and the synthetic anomaly mask. Encourages per-pixel classification accuracy.\n",
    "\n",
    "- `Focal_loss`:  \n",
    "  Focuses learning on hard pixels (e.g., near the edges of the anomaly). Reduces the effect of background pixels, which dominate due to class imbalance.\n",
    "\n",
    "‚úÖ **Combined Segmentation Loss**:  \n",
    "```python\n",
    "segmentation_loss = 0.5 * BCE_loss + 0.5 * Focal_loss\n",
    "```\n",
    "\n",
    "This hybrid loss improves the model's ability to localize anomalies precisely while handling the severe imbalance between background and anomaly pixels.\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Summary Table\n",
    "\n",
    "| Loss         | Used For        | Purpose                                       |\n",
    "|--------------|-----------------|-----------------------------------------------|\n",
    "| `L1_loss`    | Reconstruction  | Pixel-wise similarity                         |\n",
    "| `SSIM_loss`  | Reconstruction  | Perceptual similarity (structure, contrast)   |\n",
    "| `FFT_loss`   | Reconstruction  | Texture + brightness shift detection (global) |\n",
    "| `BCE_loss`   | Segmentation    | Per-pixel anomaly classification              |\n",
    "| `Focal_loss` | Segmentation    | Emphasizes hard-to-classify anomaly regions   |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Intuition\n",
    "\n",
    "By training on **augmented corruptions**, DR√ÜM learns **where** things are wrong in a robust way ‚Äî **without ever seeing real defects during training**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. What You Should Already Know\n",
    "\n",
    "If you followed the VAE and PatchCore notebooks, you're already familiar with:\n",
    "\n",
    "- PyTorch training pipelines\n",
    "- Concepts like:\n",
    "  - Patch embeddings\n",
    "  - L2 distance as anomaly measure\n",
    "  - Encoder/Decoder models\n",
    "  - Evaluation: ROC AUC, precision, recall, F1\n",
    "\n",
    "With DR√ÜM, we'll **build on that knowledge** and transition into **pixel-wise segmentation of anomalies**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Roadmap of Upcoming Notebooks\n",
    "\n",
    "| Notebook | Title | Description |\n",
    "|----------|-------|-------------|\n",
    "| **02_Data_Preparation** | Patch & Prepare AITEX | Create normal-only training set and test set with masks |\n",
    "| **03_Synthetic_Anomaly_Generation** | Simulate Anomalies | Functions for perlin noise + cut-paste augmentation |\n",
    "| **04_DRAEM_Model** | Build the DR√ÜM Model | Define U-Net and mask prediction network |\n",
    "| **05_Train_DRAEM** | Train on Normal Patches | Train the model with synthetic masks |\n",
    "| **06_Evaluate_DRAEM** | Run on Test Set | Get anomaly masks, scores, and metrics (ROC AUC, mass-center-distance, hit / miss rate) |\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs move from handcrafted descriptors and brute-force similarity ‚Äî to a **learned notion of abnormality**.\n",
    "\n",
    "‚û°Ô∏è **On to Notebook 02!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd22e3d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 0.8em; text-align: center;\">¬© 2025 Oliver Grau. Educational content for personal use only. See LICENSE.txt for full terms and conditions.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
