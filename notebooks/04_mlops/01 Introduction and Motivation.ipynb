{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292af7ca",
   "metadata": {},
   "source": [
    "# **Project: Anomaly Detection for AITEX Dataset**\n",
    "#### Track: MLOps\n",
    "## `Notebook 1`: Introduction and Motivation: Operationalization\n",
    "**Author**: Oliver Grau \n",
    "\n",
    "**Date**: 27.03.2025  \n",
    "**Version**: 1.0\n",
    "\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "- [1. Introduction](#1-introducton)\n",
    "- [2. Goals of the Notebook](#goals-of-the-notebook)\n",
    "- [3. Notebook Structure](#3-notebook-structure)\n",
    "- [4. Outlook & Next Steps](#4-outlook-and-next-steps)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "Welcome to the **Operationalization Track** of this notebook series on anomaly detection in manufacturing using image data.\n",
    "\n",
    "In the previous notebooks tracks -\n",
    "\n",
    "- `01_VAE`\n",
    "- `02_PatchCore`\n",
    "- `03_DRAEM`\n",
    "\n",
    "- we explored and implemented several powerful methods to detect anomalies in the **AITEX fabric dataset**. And we now have a trained **DRAEM-based model** that performs exceptionally well on the test set.\n",
    "\n",
    "The next step is not about improving accuracy or building new models ‚Äî it's about **bringing our best model into production.** This is the world of **MLOps**.\n",
    "\n",
    "### Key Question: What Should the MLOps Pipeline Include?\n",
    "\n",
    "Before jumping in, we face an important decision:\n",
    "\n",
    "> **Should our MLOps pipeline include everything from data preparation and training to deployment, or just the inference and monitoring part?**\n",
    "\n",
    "#### ‚úÖ Short Answer:\n",
    "\n",
    "It depends on our use case, but for most real-world **industrial applications** like fabric defect detection, the answer is:\n",
    "\n",
    "> **Focus on the inference and operationalization pipeline first**, and optionally support retraining later.\n",
    "\n",
    "#### üè≠ In the context of manufacturing:\n",
    "\n",
    "| Stage                        | Include in MLOps?       | Why / When?                                                   |\n",
    "|-----------------------------|--------------------------|----------------------------------------------------------------|\n",
    "| Data preparation            | Optional                 | Only if new data arrives regularly                             |\n",
    "| Model training              | Optional                 | Only needed for regular updates or concept drift               |\n",
    "| Evaluation / testing        | Optional (once-off)      | Often done offline and manually                                |\n",
    "| Inference pipeline          | ‚úÖ Yes                   | The core of production usage                                   |\n",
    "| Monitoring / drift detection| ‚úÖ Yes (if critical)     | Ensure input images remain within expected distribution        |\n",
    "| Logging / reproducibility   | ‚úÖ Yes                   | Track predictions and model versioning                         |\n",
    "\n",
    "In our case, training was **intensive and complex**, and it's common to perform this step **offline**. Production environments typically care about:\n",
    "- Stable inference\n",
    "- Fast response\n",
    "- Logging and monitoring\n",
    "- Reproducibility and traceability\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Goals of the Notebook\n",
    "\n",
    "In this operationalization track, we‚Äôll transform our trained DRAEM model into a usable system:\n",
    "\n",
    "- Serve predictions on new data\n",
    "- Log and track predictions + metadata\n",
    "- Manage inference configurations cleanly\n",
    "- Package everything as a deployable microservice (e.g., FastAPI)\n",
    "- Discuss model versioning, updates, and optional retraining\n",
    "\n",
    "### Planned Notebook Structure\n",
    "\n",
    "| Notebook | Title                                  | Description |\n",
    "|---------|----------------------------------------|-------------|\n",
    "| `01_Introduction and Motivation.ipynb` | üëà You are here | Motivation, scope, and roadmap |\n",
    "| `02_Inference Pipeline.ipynb`         | Inference Pipeline | Load model, run inference, visualize anomalies |\n",
    "| `03_Tracking and Config with MLflow and Wandb.ipynb`       | MLflow Tracking with W&B    | Log predictions, metrics, images, artifacts |\n",
    "| `04_Model API Deployment.ipynb`       | Serving the Model  | Wrap model in FastAPI and deploy on a service (e.g., Render) |\n",
    "| `05_(Coming soon) Training Pipeline.ipynb` | Optional Retraining| How to retrain/update the model if required |\n",
    "| `06_(Coming soon) Monitoring and Drift.ipynb`       | Monitoring & Drift | Optional: track input stats, detect data drift, send alerts |\n",
    "| `07_(Coming soon) ONNX Export and Inference.ipynb`       | Inference with ONNX | Optional: add an ONNX export + inference notebook |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754459e",
   "metadata": {},
   "source": [
    "## 3. Notebook Structure:\n",
    "\n",
    "---\n",
    "\n",
    "#### üìì `02 Inference Pipeline.ipynb` ‚Äî **Preparing Inference Logic as Reusable Code**\n",
    "\n",
    "##### ‚úÖ Goal:\n",
    "Refactor your working inference code into **modular Python files** and add some more **Post Processing**.\n",
    "\n",
    "##### üì¶ Outputs (Artifacts):\n",
    "- `/inference/` directory: with working code for inference given a full image of size 4096x256\n",
    "\n",
    "##### This notebook is **still notebook-based**, but helps you:\n",
    "- Wrap inference into functions and scripts\n",
    "- Test those functions locally\n",
    "- Lay the foundation for `04_Model API Deployment.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "#### üìì `03 Tracking and Config with MLflow and Wandb.ipynb` ‚Äî **Offline/Local Logging of Inferences**\n",
    "\n",
    "##### ‚úÖ Goal:\n",
    "Use the code from `02` to run local inference on a few samples and log:\n",
    "- input image\n",
    "- anomaly map\n",
    "- anomaly score\n",
    "- model version\n",
    "\n",
    "##### üì¶ Outputs:\n",
    "- MLflow experiment logs for your inference tests\n",
    "- Registered model (optional)\n",
    "- Set up of local MLflow tracking URI (e.g. `mlruns/`)\n",
    "\n",
    "Wrap all paths, parameters, and settings into Hydra `.yaml` files.\n",
    "\n",
    "- Choose different test images\n",
    "- Swap between model versions\n",
    "- Control thresholding behavior, etc.\n",
    "\n",
    "##### üì¶ Outputs:\n",
    "- `/inference` folder: added code for MLFlow (project file, requirements.txt etc.)\n",
    "\n",
    "##### üí° No REST service yet! Just simulate inferences **as if** they were in production. This is the ‚Äútracking-ready‚Äù test stage.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìì `04 Model API Deployment.ipynb` ‚Äî **Deploy Model as a REST API**\n",
    "\n",
    "##### ‚úÖ Goal:\n",
    "Build a `FastAPI` REST service that wraps your inference code.\n",
    "\n",
    "##### üì¶ Outputs:\n",
    "- `/service/` folder with FastAPI routes\n",
    "- `main.py` server entry\n",
    "- Render deployment notes\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Visual: Notebook Flow with Artifacts\n",
    "\n",
    "```\n",
    "01 Intro\n",
    "   ‚Üì\n",
    "02 Inference Pipeline.ipynb\n",
    "   ‚ûú infer.py, preprocess.py, model weights\n",
    "   ‚Üì\n",
    "03 Tracking with MLflow.ipynb\n",
    "   ‚ûú inference logs + MLflow setup (no service yet!)\n",
    "   ‚Üì\n",
    "04 Model API Deployment.ipynb\n",
    "   ‚ûú FastAPI + REST service wrapping the above\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Notebook | Purpose | Creates... |\n",
    "|----------|---------|------------|\n",
    "| `02` | Modularizes your working inference into files | `infer.py`, `model/`, `.pth` |\n",
    "| `03` | Tracks local inferences with MLflow (offline, no REST needed) | `mlruns/` |\n",
    "| `04` | Turns your pipeline into a REST API | `/service/`, `main.py` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95e30a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üîö Conclusion & Outlook\n",
    "\n",
    "In this notebook, we have defined how our fourth area **‚ÄúMLOps‚Äù** is structured and thus laid the foundations for the other notebooks. In the next notebook `02 Inference Pipeline.ipynb` we will design and implement a complete pipeline for the detection of anomalies based on our model.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7db9ef",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 0.8em; text-align: center;\">¬© 2025 Oliver Grau. Educational content for personal use only. See LICENSE.txt for full terms and conditions.</p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
