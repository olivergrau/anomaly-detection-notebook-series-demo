{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b490319b5426cb8",
   "metadata": {},
   "source": [
    "# **Project: Anomaly Detection for AITEX Dataset**\n",
    "#### Track: VAE\n",
    "## `Notebook 9`: Concepts and Math for VAE\n",
    "**Author**: Oliver Grau \n",
    "\n",
    "**Date**: 27.03.2025  \n",
    "**Version**: 1.0\n",
    "\n",
    "\n",
    "# **What is a Variational Autoencoder (VAE)?**\n",
    "\n",
    "# üìö Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [What is a Variational Autoencoder (VAE) and what problem does it solve?](#what-is-a-variational-autoencoder-vae-and-what-problem-does-it-solve)\n",
    "- [Core Idea of VAE](#core-idea)\n",
    "- [The Mathematics of a VAE](#the-mathematics-of-a-vae)\n",
    "  - [Latent Distributions: Mean and Standard Deviation](#latent-distributions-mean-and-standard-deviation)\n",
    "  - [Loss Function](#loss-function)\n",
    "- [Why is this useful for Anomaly Detection?](#why-is-this-useful-for-anomaly-detection)\n",
    "- [Difference between a Normal Autoencoder and a VAE](#difference-between-a-normal-autoencoder-and-a-vae)\n",
    "- [Why Use a Distribution Instead of a Direct Vector?](#why-use-a-distribution-instead-of-a-direct-vector)\n",
    "- [The Reparameterization Trick](#the-reparameterization-trick)\n",
    "- [What is KL Divergence Doing?](#what-is-kl-divergence-doing)\n",
    "- [Summary: Why This Matters for Anomaly Detection](#summary-why-this-matters-for-anomaly-detection)\n",
    "- [Generative Models and Anomaly Detection](#generative-models-and-anomaly-detection)\n",
    "  - [What Does \"Generative\" Mean?](#what-does-generative-mean)\n",
    "  - [Anomaly Detection with Generative Models](#anomaly-detection-with-generative-models)\n",
    "  - [Concrete Example](#concrete-example)\n",
    "  - [Key Insight](#key-insight)\n",
    "- [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we dive deep into the **core concepts and mathematics** behind **Variational Autoencoders (VAEs)**, focusing specifically on how they enable **anomaly detection** in highly regular data such as **fabric textures**.\n",
    "\n",
    "We will explore:\n",
    "- Why VAEs model **distributions** instead of fixed encodings\n",
    "- How the **Reparameterization Trick** allows gradient-based optimization\n",
    "- The role of **KL Divergence** in shaping a meaningful latent space\n",
    "- And most importantly: **How the generative power of VAEs helps detect anomalies** through reconstruction errors.\n",
    "\n",
    "This is not just a theoretical journey. Every concept is linked back to **real-world anomaly detection**, preparing you to deeply understand **why VAEs work or not** and **how to use them effectively or not**.\n",
    "\n",
    "---\n",
    "\n",
    "A **Variational Autoencoder (VAE)** is a type of **generative model**. It learns to approximate the **distribution** of your input data. In your case: clean, **defect-free grayscale fabric patches**.\n",
    "\n",
    "## Core Idea of VAE:\n",
    "> \"Learn what *normal* fabric looks like, so that you can detect when something is *not normal* (an anomaly).\"\n",
    "\n",
    "VAEs consist of two main components:\n",
    "\n",
    "1. **Encoder**: Compresses the input image $ x $ into a **distribution** over latent variables $ z $.  \n",
    "2. **Decoder**: Reconstructs the original image $ x $ from a sampled $ z \\sim q(z|x) $.\n",
    "\n",
    "---\n",
    "\n",
    "### The Mathematics of a VAE\n",
    "\n",
    "Unlike a regular Autoencoder (which gives you a fixed encoding $ z = f(x) $), the VAE encoder outputs **two vectors**:\n",
    "\n",
    "- $ \\mu(x) $: Mean of the latent distribution\n",
    "- $ \\sigma(x) $: Standard deviation of the latent distribution\n",
    "\n",
    "Then it samples:\n",
    "\n",
    "$$\n",
    "z = \\mu(x) + \\sigma(x) \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "‚Üí This is called the **reparameterization trick**.\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The total VAE loss is made up of two parts:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}}(x) = \\underbrace{\\mathbb{E}_{q(z|x)} [\\log p(x|z)]}_{\\text{Reconstruction Loss}} - \\underbrace{D_{\\text{KL}}(q(z|x) \\parallel p(z))}_{\\text{KL Divergence}}\n",
    "$$\n",
    "\n",
    "Or more commonly written for optimization:\n",
    "\n",
    "$$\n",
    "\\text{Loss}(x) = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\parallel \\mathcal{N}(0, I))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- The **Reconstruction Loss** is usually **MSE (Mean Squared Error)** or **BCE (Binary Cross Entropy)** ‚Äî it ensures that the output looks like the input.\n",
    "- The **KL Divergence** forces the encoder‚Äôs output distributions to stay close to a standard normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is this useful for Anomaly Detection?\n",
    "\n",
    "Because your fabric images are **highly regular**, the VAE can **efficiently learn the structure of normal images**.\n",
    "\n",
    "So when you input an **anomalous patch** (e.g. a thread defect or stain):\n",
    "\n",
    "- The encoder struggles to represent it as a \"normal\" latent variable.\n",
    "- The decoder **fails to reconstruct** the image well.\n",
    "- ‚áí This leads to a **high reconstruction error**, which you interpret as an **anomaly score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Your Fabric Images\n",
    "\n",
    "VAEs will learn:\n",
    "\n",
    "> \"Normal fabrics have repetitive patterns, regular textures, straight lines...\"\n",
    "\n",
    "When something **breaks that regularity** (e.g., missing threads, irregular weave), the model won‚Äôt know how to reconstruct it, and that **failure becomes your anomaly signal**.\n",
    "\n",
    "\n",
    "Let's dive deeper into the **KL Divergence**, **Reparameterization**, and **why VAEs use distributions (mean & std)** instead of direct latent vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs the key difference between a normal Autoencoder and a VAE?\n",
    "\n",
    "| Type | Latent Representation                                      |\n",
    "|------|------------------------------------------------------------|\n",
    "| Autoencoder | Deterministic: $ z = f(x) $                                |\n",
    "| VAE | Probabilistic: $ z \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x)) $ |\n",
    "\n",
    "Instead of learning a **single point in latent space**, the VAE learns a **distribution** ‚Äì specifically, a Gaussian with **mean** $ \\mu $ and **standard deviation** $ \\sigma $.\n",
    "\n",
    "---\n",
    "\n",
    "## Why use a distribution (mean & std) instead of a direct vector?\n",
    "\n",
    "### Intuition:\n",
    "When using a single point:\n",
    "- The model may **overfit** the training data.\n",
    "- The latent space becomes **disjointed** ‚Äî there's no smooth transition between similar images.\n",
    "\n",
    "By using a distribution:\n",
    "- The VAE encourages the latent space to be **continuous and smooth**.\n",
    "- It becomes possible to **generate new samples** by drawing from a normal distribution $ \\mathcal{N}(0, I) $.\n",
    "- This makes the model **generative** ‚Äî it can create new, plausible data from noise!\n",
    "\n",
    "---\n",
    "\n",
    "## The Reparameterization Trick\n",
    "\n",
    "The problem: we need to **sample** from a distribution during training, but sampling is **non-differentiable**, which breaks backpropagation.\n",
    "\n",
    "The solution:  \n",
    "Use the **reparameterization trick**:\n",
    "\n",
    "Instead of sampling directly from $ z \\sim \\mathcal{N}(\\mu, \\sigma^2) $, we do this:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Now, $ \\mu $ and $ \\sigma $ are part of a **deterministic computation graph** (differentiable), and the randomness is isolated in $ \\epsilon $.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè What is KL Divergence doing?\n",
    "\n",
    "KL Divergence measures how \"far apart\" two distributions are.\n",
    "\n",
    "In a VAE, it compares:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q(z|x) \\;||\\; p(z))\n",
    "$$\n",
    "\n",
    "- $ q(z|x) $: The **learned** distribution (from the encoder).\n",
    "- $ p(z) $: A **fixed prior** distribution ‚Äî usually a standard normal $ \\mathcal{N}(0, I) $.\n",
    "\n",
    "### Why penalize this divergence?\n",
    "\n",
    "We **force** the learned latent distributions to stay close to the prior. That way:\n",
    "\n",
    "- The **latent space stays well-organized**.\n",
    "- We can sample **anywhere in latent space**, and the decoder will still produce realistic outputs.\n",
    "\n",
    "### The KL Term (Analytic Expression):\n",
    "\n",
    "If both $ q(z|x) $ and $ p(z) $ are Gaussians:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(\\mathcal{N}(\\mu, \\sigma^2) \\;||\\; \\mathcal{N}(0, 1)) = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n",
    "$$\n",
    "\n",
    "This can be implemented easily in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this all matters for anomaly detection\n",
    "\n",
    "- By enforcing a **smooth latent space**, the model learns a strong **prior over normal data**.\n",
    "- **Anomalous inputs** (e.g., defects in fabric) do **not follow this prior** well ‚Üí they result in poor reconstructions.\n",
    "- So, **high reconstruction error** = likely anomaly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73e6d987e6159a",
   "metadata": {},
   "source": [
    "In **anomaly detection**, you're **not using the VAE to generate images for fun**, like a GAN generating cat pictures. Instead, you want to use its **generative capacity** in a very specific way.\n",
    "\n",
    "Let‚Äôs unpack this step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## What does ‚Äúgenerative‚Äù mean?\n",
    "\n",
    "A **generative model** is one that can **learn the distribution of a dataset** and then **generate new samples** that \"look like\" the training data.\n",
    "\n",
    "For a VAE, this means:\n",
    "\n",
    "- During training, it learns a mapping from inputs $ x $ (your fabric patches) to a latent space $ z $ (via $ q(z|x) $)\n",
    "- This latent space is forced (via KL loss) to look like a **standard normal distribution** $ \\mathcal{N}(0, I) $\n",
    "- After training, you can sample a random vector $ z \\sim \\mathcal{N}(0, I) $ and feed it into the decoder\n",
    "- The decoder then generates a **new, plausible image** that resembles your training data\n",
    "\n",
    "‚û°Ô∏è **So the model learns to map \"noise\" (random vector $ z $) into realistic outputs.**\n",
    "\n",
    "---\n",
    "\n",
    "## In the context of your project: why does this matter?\n",
    "\n",
    "You‚Äôre not **sampling random noise** to create new fabrics.  \n",
    "You're using this **generative ability in reverse**:\n",
    "\n",
    "### üö® Anomaly Detection with Generative Models:\n",
    "- You feed a possibly **defective image patch** into the VAE.\n",
    "- The VAE encodes it into a latent distribution.\n",
    "- It samples from that distribution and decodes it ‚Üí giving a **reconstructed version of what the model thinks is \"normal\"**.\n",
    "\n",
    "If the input was a **normal fabric**, the reconstruction will be **very accurate**.\n",
    "\n",
    "If the input was **anomalous**:\n",
    "- The VAE **never saw such defects** during training.\n",
    "- Its decoder will reconstruct what it **thinks should have been there**.\n",
    "- The result will look like a **\"corrected\" version** of the patch.\n",
    "- You compare the input and the output ‚Üí **difference = anomaly score**\n",
    "\n",
    "---\n",
    "\n",
    "### üì∏ Concrete Example:\n",
    "\n",
    "Let‚Äôs say the original input patch looks like this:\n",
    "\n",
    "```\n",
    "Input image:     Patch with a broken thread (defect)\n",
    "Latent z:        Sample from encoder (conditioned on input)\n",
    "Output image:    Patch without the broken thread (model's guess of normal)\n",
    "```\n",
    "\n",
    "If the input is:\n",
    "\n",
    "- Normal ‚Üí model reconstructs it well ‚Üí low error\n",
    "- Abnormal ‚Üí model reconstructs it as if **nothing was wrong** ‚Üí large error at the defect region\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "Even though you're feeding in **real input images**, the decoder‚Äôs job is **not to memorize or copy**.\n",
    "\n",
    "It‚Äôs to **guess the most likely \"normal version\"** of that input **based on everything it learned**.\n",
    "\n",
    "This ability to **infer what should be there** is **what makes the model generative**, and it's **why it works so well for anomaly detection**. At least if the anomalies are really anomal and not tiny.\n",
    "\n",
    "---\n",
    "\n",
    "A VAE is called \"generative\" because, once trained, it can take random vectors from latent space and create realistic images. In anomaly detection, we **use this to reconstruct the \"normal\" version** of a possibly defective image and then detect discrepancies between the reconstruction and original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8830a6beb172ae",
   "metadata": {},
   "source": [
    "Let‚Äôs **separate two things** that often cause confusion:\n",
    "\n",
    "---\n",
    "\n",
    "## Two modes of using a VAE:\n",
    "\n",
    "### 1. **Sampling Mode (generative use case)**\n",
    "\n",
    "- You **sample a random latent vector** $ z \\sim \\mathcal{N}(0, I) $\n",
    "- Pass it through the **decoder**\n",
    "- Output: a **completely new image** that *looks like* your training data\n",
    "\n",
    "‚úÖ This is the *true generative use case*. Like what GANs or VAEs use for **creative generation**\n",
    "\n",
    "> You‚Äôre not using this mode in your anomaly detection setup.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Reconstruction Mode (your use case)**\n",
    "\n",
    "- You **input a real image** $ x $\n",
    "- The **encoder** maps it to $ \\mu(x) $ and $ \\sigma(x) $ ‚Üí a distribution over latent codes\n",
    "- You sample $ z \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x)) $\n",
    "- The **decoder** reconstructs the image: $ \\hat{x} = p(x|z) $\n",
    "\n",
    "‚úÖ This is what you're doing for **anomaly detection**\n",
    "\n",
    "> Here, the input image is **always** the starting point.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ So why is it still called ‚Äúgenerative‚Äù?\n",
    "\n",
    "Even though you're not generating random samples, the **decoder itself has learned how to ‚Äúgenerate‚Äù the normal-looking version of any latent code**.\n",
    "\n",
    "And by **training it with a KL loss**, you made sure that the latent space is:\n",
    "\n",
    "- Smooth\n",
    "- Well-organized\n",
    "- Close to a standard Gaussian\n",
    "\n",
    "So **in theory**, you *could* sample a random $ z \\sim \\mathcal{N}(0, I) $ and get a realistic output. That‚Äôs why it‚Äôs still called a **generative model** ‚Äî it has this potential, even if you don‚Äôt use it that way.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç In practice: reconstruction from input\n",
    "\n",
    "You can think of it this way:\n",
    "\n",
    "> ‚ÄúI feed the model a possibly defective image, and it *imagines* what the most probable normal version of that image would be based on everything it has learned from normal data.‚Äù\n",
    "\n",
    "That‚Äôs the **power of the generative decoder**:  \n",
    "It doesn't copy the input. It tries to \"guess what should be there.\"\n",
    "\n",
    "And if there‚Äôs something strange (a defect), it often **fails to reconstruct that part correctly**, leading to a **large reconstruction error ‚Üí anomaly detected**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Term              | Meaning in your context                                                                                    |\n",
    "|-------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| Input image       | The actual 256√ó256 fabric patch                                                                            |\n",
    "| Latent code $ z $ | Sampled from encoder output distribution $ \\mu(x), \\sigma(x) $                                             |\n",
    "| Decoder           | Tries to reconstruct what a ‚Äúnormal‚Äù version of the input would look like                                  |\n",
    "| Generative        | Refers to decoder‚Äôs ability to output plausible data based on latent $ z $ ‚Äî even without a specific input |\n",
    "\n",
    "The input image is the starting point in your anomaly detection setup and the \"generative\" part is what enables the decoder to guess what should be there based on learned regularities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728bfe15b701b5f",
   "metadata": {},
   "source": [
    "Let‚Äôs now dive into the **KL Divergence** ‚Äî both **conceptually** and **mathematically** ‚Äî and see exactly **why it appears in a VAE**, what it **does**, and **how to compute it**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is KL Divergence?\n",
    "\n",
    "The **Kullback-Leibler (KL) divergence** is a measure of how one probability distribution **differs** from another.\n",
    "\n",
    "Formally, for two probability distributions $ q(z) $ and $ p(z) $:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q(z) \\,\\|\\, p(z)) = \\int q(z) \\log \\frac{q(z)}{p(z)} \\, dz\n",
    "$$\n",
    "\n",
    "It tells you how much **information is lost** when you approximate $ p(z) $ using $ q(z) $.  \n",
    "It is **not symmetric**: $ D_{\\text{KL}}(q \\,\\|\\, p) \\neq D_{\\text{KL}}(p \\,\\|\\, q) $\n",
    "\n",
    "---\n",
    "\n",
    "## Why do we use KL Divergence in VAEs?\n",
    "\n",
    "In a VAE, the **encoder** learns a distribution $ q(z|x) $ (typically a Gaussian with mean and variance).  \n",
    "But we want this distribution to be **close to a fixed prior**, usually:\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "So we penalize the **difference** between the learned distribution \\( q(z|x) \\) and this prior using KL divergence:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{Reconstruction Error} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n",
    "$$\n",
    "\n",
    "This forces the encoder to produce **latent spaces that are organized, continuous, and close to a standard Gaussian**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Math: KL Between Two Gaussians\n",
    "\n",
    "Assume:\n",
    "\n",
    "- $ q(z|x) = \\mathcal{N}(\\mu, \\sigma^2) $\n",
    "- $ p(z) = \\mathcal{N}(0, 1) $\n",
    "\n",
    "Then the KL divergence has a **closed-form solution**:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z)) = \\frac{1}{2} \\sum_{i=1}^d \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu_i $: mean of latent dimension $ i $\n",
    "- $ \\sigma_i $: stddev of latent dimension $ i $\n",
    "- $ d $: dimensionality of the latent space (e.g. 16, 32‚Ä¶)\n",
    "\n",
    "This is **simple to compute** in PyTorch and fully differentiable.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition of each term:\n",
    "\n",
    "$$\n",
    "\\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1\n",
    "$$\n",
    "\n",
    "| Term                  | Intuition |\n",
    "|-----------------------|----------|\n",
    "| $ \\mu_i^2 $           | Penalizes moving the center of the distribution away from 0 |\n",
    "| $ \\sigma_i^2 $        | Penalizes making the distribution too wide |\n",
    "| $ -\\log(\\sigma_i^2) $ | Penalizes making it too narrow |\n",
    "| $ -1 $                | Constant offset for proper scaling |\n",
    "\n",
    "So the KL loss:\n",
    "- Keeps the encoder‚Äôs output distribution centered around zero\n",
    "- Prevents it from collapsing into a **deterministic** encoder (where $ \\sigma \\to 0 $)\n",
    "- Keeps the **entire latent space ‚Äúused‚Äù** and **smooth**\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Intuition\n",
    "\n",
    "Imagine each training input creates a **small Gaussian cloud** in latent space.  \n",
    "The KL loss makes sure that:\n",
    "\n",
    "- All clouds **overlap well**\n",
    "- They don‚Äôt wander off into the corners\n",
    "- They **match the shape** of the big, round standard normal cloud\n",
    "\n",
    "This helps ensure that the latent space is **compact**, **smooth**, and **well-behaved**. This is crucial for both reconstruction and generative capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## PyTorch Example\n",
    "\n",
    "```python\n",
    "# Assume mu and logvar come from the encoder\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "```\n",
    "\n",
    "- $ \\text{logvar} = \\log(\\sigma^2) $ is often predicted by the encoder for numerical stability\n",
    "- This function works per batch, summing across all latent dimensions\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary\n",
    "\n",
    "| Concept | Meaning                                                                    |\n",
    "|--------|----------------------------------------------------------------------------|\n",
    "| **KL Divergence** | Measures how far the encoder's distribution is from the standard Gaussian  |\n",
    "| **Why it's used** | Ensures the latent space is smooth, compact, and similar for all inputs    |\n",
    "| **What it does** | Regularizes the encoder and keeps $ \\mu \\approx 0 $, $ \\sigma \\approx 1 $    |\n",
    "| **Practical form** | Simple closed-form expression with $ \\mu $ and $ \\sigma $ from the encoder |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256d8e22676cf17",
   "metadata": {},
   "source": [
    "Great! Let‚Äôs dust off the Fourier Toolbox and make this intuitive again! We‚Äôll compare **pixel-wise loss** and **frequency-based loss** in the context of **anomaly detection** with VAEs, especially for **fabric textures** like in the AITEX dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pixel-wise Loss (e.g. MSE)\n",
    "\n",
    "This is the **classic reconstruction loss**. You‚Äôre simply comparing the **reconstructed image** $ \\hat{x} $ to the **original image** $ x $, pixel by pixel:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{pixel}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( x_i - \\hat{x}_i \\right)^2\n",
    "$$\n",
    "\n",
    "- Measures local differences in intensity\n",
    "- Penalizes **per-pixel** errors equally\n",
    "- Very effective when images have sharp features (e.g., digits, faces)\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Simple and widely used\n",
    "- Works well for images where anomalies are visible as pixel-level changes\n",
    "\n",
    "### ‚ùå Cons (especially for textures):\n",
    "- **Insensitive to structural patterns**\n",
    "- Can miss **global pattern irregularities** in textures (like fabrics)\n",
    "- Can allow **blurry reconstructions** that \"fool\" the loss even if the pattern is broken\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Frequency-based Loss (e.g. FFT loss)\n",
    "\n",
    "This loss compares **differences in the frequency domain**, rather than pixel-by-pixel. You transform both the input and reconstructed image using the **Fourier Transform (FT)**:\n",
    "\n",
    "$$\n",
    "X_f = \\text{FFT}(x), \\quad \\hat{X}_f = \\text{FFT}(\\hat{x})\n",
    "$$\n",
    "\n",
    "Then compute the loss (e.g. L1 or L2) between **magnitude spectra**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{freq}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left|\\, |X_{f, i}| - |\\hat{X}_{f, i}| \\,\\right|^2\n",
    "$$\n",
    "\n",
    "Note: often, we **only use the magnitudes** and ignore the phase.\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Captures **global structure**, regularity, and periodicity\n",
    "- Very sensitive to **changes in pattern frequency** (e.g. broken weave, missing lines)\n",
    "- Much better suited for **fabric-like textures**\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- May ignore small local anomalies (e.g. small stains)\n",
    "- Ignores phase (i.e., location) unless explicitly preserved\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Combining Both Losses\n",
    "\n",
    "In practice, **combining both pixel-wise and frequency loss** works very well:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{pixel}} \\cdot \\mathcal{L}_{\\text{pixel}} + \\lambda_{\\text{freq}} \\cdot \\mathcal{L}_{\\text{freq}}\n",
    "$$\n",
    "\n",
    "Where $ \\lambda $ are weights to balance the two.  \n",
    "This gives you the best of both worlds:\n",
    "\n",
    "| Type | Captures |\n",
    "|------|----------|\n",
    "| Pixel loss | Local image details |\n",
    "| Frequency loss | Global, periodic structure & texture |\n",
    "\n",
    "For AITEX, where patterns are **highly repetitive**, the **frequency component is crucial**.\n",
    "\n",
    "---\n",
    "\n",
    "## Refresher: What‚Äôs in the Frequency Domain?\n",
    "\n",
    "When you apply a **2D FFT** to an image, the result tells you:\n",
    "\n",
    "- **What frequencies are present** (how often things repeat)\n",
    "- **How strong they are** (intensity of those patterns)\n",
    "- The center of the FFT image contains **low frequencies** (smooth gradients)\n",
    "- The outer edges contain **high frequencies** (sharp edges, fine textures)\n",
    "\n",
    "In fabrics:\n",
    "- Regular stripes, grids ‚Üí show as strong **peaks** in frequency space\n",
    "- Anomalies ‚Üí distort these peaks or introduce irregular ones\n",
    "\n",
    "---\n",
    "\n",
    "## PyTorch Snippet (Magnitude Spectrum)\n",
    "\n",
    "Here‚Äôs a tiny example of how to implement frequency loss in PyTorch:\n",
    "\n",
    "```python\n",
    "def fft_magnitude(img):\n",
    "    # Assumes img is (B, 1, H, W)\n",
    "    fft = torch.fft.fft2(img)\n",
    "    return torch.abs(fft)\n",
    "\n",
    "def frequency_loss(x, x_recon):\n",
    "    x_f = fft_magnitude(x)\n",
    "    x_recon_f = fft_magnitude(x_recon)\n",
    "    return torch.mean((x_f - x_recon_f) ** 2)\n",
    "```\n",
    "\n",
    "Then combine it with MSE:\n",
    "\n",
    "```python\n",
    "loss = pixel_loss(x, x_recon) + 0.1 * frequency_loss(x, x_recon)\n",
    "```\n",
    "\n",
    "The `0.1` is a hyperparameter you‚Äôll want to tune for your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary\n",
    "\n",
    "| Loss | Good For | Weakness |\n",
    "|------|----------|----------|\n",
    "| **Pixel-wise (MSE)** | Local intensity differences | Misses global patterns |\n",
    "| **Frequency-based** | Repetitive textures, structural regularity | Misses small, localized defects |\n",
    "| **Combined** | Balanced sensitivity | Best for fabrics like AITEX |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86c568",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "Through this notebook, we've unpacked the **mathematical foundations** and **intuitive insights** that make VAEs such powerful tools for anomaly detection.\n",
    "\n",
    "Key takeaways:\n",
    "- VAEs learn a **probabilistic latent space**, enabling them to generate plausible reconstructions of normal data.\n",
    "- The **KL Divergence** ensures the latent space is **structured and smooth**, critical for generalization.\n",
    "- Anomalies are detected by observing **large reconstruction errors**, as the VAE struggles to recreate data it was never trained to model.\n",
    "\n",
    "Understanding these principles empowers you to not just **apply VAEs**, but also **adapt and improve** them for your specific anomaly detection tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26b782",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 0.8em; text-align: center;\">¬© 2025 Oliver Grau. Educational content for personal use only. See LICENSE.txt for full terms and conditions.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
