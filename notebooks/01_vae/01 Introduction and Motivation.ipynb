{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54adfea9ab50aa7",
   "metadata": {},
   "source": [
    "# **Project: Anomaly Detection for AITEX Dataset**\n",
    "#### Track: VAE\n",
    "## `Notebook 1`: Anomaly Detection with Autoencoders: Introduction & Motivation\n",
    "**Author**: Oliver Grau \n",
    "\n",
    "**Date**: 27.03.2025  \n",
    "**Version**: 1.0\n",
    "\n",
    "## Why Anomaly Detection Matters Across Enterprises\n",
    "\n",
    "In today‚Äôs data-driven organizations, detecting anomalies is a critical part of ensuring **operational resilience, security, and quality control**. Whether it‚Äôs predictive maintenance, fraud prevention, network security, or process monitoring. Enterprises rely on the ability to identify **subtle deviations from normal behavior** that may indicate underlying issues or risks.\n",
    "\n",
    "While rule-based systems or classical thresholding may struggle with complex, high-dimensional data, machine learning methods - particularly **reconstruction-based models like Autoencoders** - provide a scalable, data-adaptive alternative.\n",
    "\n",
    "This notebook series focuses on how Autoencoders can be used for **unsupervised anomaly detection**, making them well-suited for settings where labeled anomalies are rare, expensive, or unavailable.\n",
    "\n",
    "---\n",
    "\n",
    "## Target Audience\n",
    "\n",
    "This notebook is designed for **data scientists, ML engineers, and AI practitioners** who already understand the fundamentals of neural networks and machine learning. The goal is to provide a **practical, implementation-focused guide** for applying Autoencoders to anomaly detection scenarios in real-world enterprise environments.\n",
    "\n",
    "Rather than repeating introductory theory, we focus on design decisions, best practices, and common pitfalls and all supported by real data and clear visualizations. But of course some mathematical theory is always beneficial, so feel free to consult `09_Bonus - Concepts and Math.ipynb` for a  mathematical foundation to autoencoders.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Enterprise Use Cases for Anomaly Detection\n",
    "\n",
    "- **Predictive Maintenance in Industrial Systems**: Detect mechanical faults before failure by analyzing multivariate sensor data.\n",
    "- **Fraud & Abuse Detection**: Identify outlier patterns in financial transactions, platform usage, or authentication logs.\n",
    "- **IT Infrastructure & Security**: Detect unusual system behavior, network intrusions, or performance bottlenecks.\n",
    "- **Quality Assurance & Process Monitoring**: Spot visual or sensor anomalies in production pipelines, especially in manufacturing and healthcare.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Autoencoders?\n",
    "\n",
    "Autoencoders are a natural fit for anomaly detection because they are trained to **reconstruct normal data**. When faced with anomalous input, they typically fail to reconstruct it well resulting in a **higher reconstruction error**, which we can use as an anomaly signal.\n",
    "\n",
    "They offer several advantages:\n",
    "\n",
    "- They work well in **unsupervised settings** (no need for labeled anomalies)\n",
    "- They are adaptable to **tabular, image, time-series, or mixed data**\n",
    "- They are **interpretable and modular**, making them easy to integrate and extend in production environments\n",
    "\n",
    "---\n",
    "\n",
    "## What You‚Äôll Build in This first part of the Notebook series\n",
    "\n",
    "By following along, you will:\n",
    "\n",
    "- Implement a PyTorch-based Autoencoder tailored for anomaly detection\n",
    "- Train the model on a real or representative dataset\n",
    "- Evaluate anomalies using reconstruction error\n",
    "- Explore thresholds, visualizations, and result interpretation\n",
    "\n",
    "This forms the baseline for more advanced extensions, including cloud deployment, real-time inference, or integration into enterprise systems.\n",
    "\n",
    "---\n",
    "\n",
    "# üìö Table of Contents\n",
    "\n",
    "## 1. Introduction & Motivation\n",
    "- Why anomaly detection matters for enterprises\n",
    "- Real-world use cases (predictive maintenance, fraud, IT, QA)\n",
    "- From examples to abstraction: what is an anomaly?\n",
    "\n",
    "## 2. Understanding Autoencoders\n",
    "- What is an Autoencoder? (Intuitive explanation)\n",
    "- Key components: Encoder, Decoder, Bottleneck\n",
    "- Why Autoencoders are useful for anomaly detection\n",
    "- Visual example with toy data and code\n",
    "\n",
    "## 3. Input Features: What Kind of Data Works?\n",
    "- Tabular vs. time-series vs. image data\n",
    "- How to structure and normalize input features\n",
    "- Why reconstruction-based models need stable normal data\n",
    "- Examples from different domains (sensor logs, metrics, images)\n",
    "\n",
    "## 4. Dataset: AITEX Overview\n",
    "- Background on the dataset (real-world relevance)\n",
    "- Structure and features of the data\n",
    "- Why this dataset is suitable for our first model\n",
    "- Optional: link to enterprise-relevant analogies (e.g., sensor monitoring, transactions)\n",
    "\n",
    "## 5. Data Preparation & Exploration\n",
    "- Loading and inspecting the dataset\n",
    "- Exploratory data analysis (EDA)\n",
    "  - Missing values\n",
    "  - Time-series structure\n",
    "  - Visualization of normal vs. anomalous samples\n",
    "- Preprocessing steps (e.g., normalization, train/test split)\n",
    "\n",
    "## 6. Building the Autoencoder\n",
    "- PyTorch model definition (step-by-step)\n",
    "- Choosing architecture & activation functions\n",
    "- Loss function (MSE) and optimizer setup\n",
    "- Explanation of design decisions in simple language\n",
    "\n",
    "## 7. Training the Model\n",
    "- Training loop with progress visualization\n",
    "- Evaluation of reconstruction loss on training data\n",
    "- Optional: early stopping or validation logic\n",
    "\n",
    "## 8. Why the VAE struggles with AITEX Anomaly Detection\n",
    "- How to interpret reconstruction error\n",
    "- Threshold selection (manual vs. statistical)\n",
    "- Visualizing anomalies vs. normal data\n",
    "- Precision/Recall (light introduction, intuitive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c69f9d001b08e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"border-left: 4px solid #007acc; padding: 0.8em; background-color: #f0f8ff; margin-bottom: 1em;\">\n",
    "  <strong>üí° Reader Question:</strong> <br><br>\n",
    "  We have <b>labels</b> (binary masks) in our AITEX dataset, so we could use <b>supervised</b> classification + segmentation methods instead of <b>unsupervised learning</b>?\n",
    "</div>\n",
    "\n",
    "# When to Use Supervised Classification + Segmentation\n",
    "\n",
    "This approach works *really well* when:\n",
    "- You **have labeled data**: ground truth labels for each patch (\"defect\" or \"no defect\"), and ideally, segmentation masks.\n",
    "- Your **defect types are known** and represented in the training set.\n",
    "- You're okay with training a **specific model per dataset** or defect type.\n",
    "\n",
    "That‚Äôs why the classification + segmentation pipeline worked so well. The AITEX dataset has clearly labeled defective and non-defective patches, and labeled segmentation masks.\n",
    "\n",
    "But...\n",
    "\n",
    "\n",
    "## ‚ùì**What If Labels Are Scarce or Unavailable?**\n",
    "\n",
    "That‚Äôs where unsupervised anomaly detection methods shine:\n",
    "\n",
    "### 1. **Autoencoders (VAEs, CNN-AEs)**\n",
    "- Train **only on normal images** (i.e., no defects).\n",
    "- Learn to reconstruct these normal inputs.\n",
    "- At test time, any **high reconstruction error** ‚Üí likely anomaly.\n",
    "- Good for **unexpected defects** or when **no defect masks are available**.\n",
    "\n",
    "### 2. **PatchCore**\n",
    "- Self-supervised: builds a **memory bank of normal patch features**.\n",
    "- At test time, compares test patches to the memory bank (using k-NN in feature space).\n",
    "- Doesn‚Äôt need defect annotations.\n",
    "- Excels at **catching subtle deviations** from normal texture, especially in regular patterns (like fabric, PCB, etc.).\n",
    "\n",
    "### 3. **DR√ÜM (Discriminative AE for Anomaly Detection)**\n",
    "- Trains with **synthetic anomalies** (noise masks or inpainted regions).\n",
    "- Learns to distinguish between \"real\" images and \"tampered\" ones.\n",
    "- Outputs **pixel-level anomaly maps**, again using **only normal data**.\n",
    "- Very strong for **pixel-wise localization** without labeled defects.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Summary Table\n",
    "\n",
    "| Approach                         | Needs Labels? | Detect Unknown Defects? | Pixel-wise Output? | Data Efficiency | When to Use |\n",
    "|----------------------------------|---------------|--------------------------|---------------------|------------------|--------------|\n",
    "| **Classification + Segmentation** | ‚úÖ Yes         | ‚ùå No                   | ‚úÖ Yes              | ‚ùå Needs many labeled images | You have good, labeled datasets (e.g., AITEX) |\n",
    "| **Autoencoder (VAE)**             | ‚ùå No          | ‚úÖ Yes                  | ‚ùå Optional         | ‚úÖ Very data-efficient       | Defects are unknown, labels are scarce |\n",
    "| **PatchCore**                     | ‚ùå No          | ‚úÖ Yes                  | ‚úÖ Yes              | ‚úÖ Very data-efficient       | High regularity, unknown anomalies |\n",
    "| **DR√ÜM**                          | ‚ùå No          | ‚úÖ Yes                  | ‚úÖ Yes              | ‚úÖ Very data-efficient       | You want localization, but no labels |\n",
    "\n",
    "---\n",
    "\n",
    "## So, Why Use (V)AE / PatchCore / DR√ÜM?\n",
    "\n",
    "Because in **real industrial scenarios**, the ideal labeled dataset is **rare**:\n",
    "- No one has time to label every single defect pixel.\n",
    "- You often care about **novel/unseen defect types**.\n",
    "- You want a model that can say: _‚ÄúThis doesn‚Äôt look like normal fabric at all‚Äù_ ‚Äî even if it‚Äôs never seen that type of defect.\n",
    "\n",
    "**Unsupervised = robustness + label freedom + generalization.**\n",
    "\n",
    "Want a metaphor?  \n",
    "\n",
    "**Supervised classification + segmentation** is like a doctor trained to recognize known diseases from textbook images.  \n",
    "**Autoencoders, PatchCore and DRAEM** are like a doctor trained to understand \"what healthy looks like\" and spot **anything** that's off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e78782",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 0.8em; text-align: center;\">¬© 2025 Oliver Grau. Educational content for personal use only. See LICENSE.txt for full terms and conditions.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
