{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4f4b89aaa1a63c",
   "metadata": {},
   "source": [
    "# High-Level VAE Concepts for AitexVAE (version 1)\n",
    "\n",
    "A VAE consists of:\n",
    "\n",
    "1. **Encoder**: Maps the input $ x $ to parameters of a distribution $ q_\\phi(z|x) $ over latent variable $ z $\n",
    "2. **Latent space sampling**: Sample $ z \\sim q_\\phi(z|x) = \\mathcal{N}(\\mu, \\sigma^2) $ via the reparameterization trick.\n",
    "3. **Decoder**: Maps $ z $ to a reconstruction $ \\hat{x} $, which models $ p_\\theta(x|z) $.\n",
    "4. **Loss function**: Combines\n",
    "   - **Reconstruction loss** (e.g., MSE or BCE): Measures how close $ \\hat{x} $ is to $ x $\n",
    "   - **KL divergence**: Regularizes $ q_\\phi(z|x) $ to be close to the prior $ p(z) = \\mathcal{N}(0, I) $\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\text{KL}(q_\\phi(z|x) \\,\\|\\, p(z))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Now Let‚Äôs Map This to Your `AitexVAE` Class\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. **Encoder**\n",
    "\n",
    "```python\n",
    "self.encoder = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1), ...\n",
    "    ...\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(), nn.Dropout2d(dropout_p),\n",
    "    nn.Flatten()\n",
    ")\n",
    "```\n",
    "\n",
    "- Each `Conv2d` reduces spatial size by half: $ 256 \\rightarrow 128 \\rightarrow 64 \\rightarrow 32 \\rightarrow 16 $\n",
    "- Channel depth increases to extract richer features: $ 1 \\rightarrow 32 \\rightarrow 64 \\rightarrow 128 \\rightarrow 256 $\n",
    "- `Flatten()` converts the feature map into a 1D vector for the fully connected layers.\n",
    "\n",
    "‚û°Ô∏è This corresponds to **inferring a latent distribution** over the code $ z $ from input $ x $.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. **Latent Space (Mean & Log-Variance)**\n",
    "\n",
    "```python\n",
    "self.fc_mu = nn.Linear(self.feature_map_dim, latent_dim)\n",
    "self.fc_logvar = nn.Linear(self.feature_map_dim, latent_dim)\n",
    "```\n",
    "\n",
    "- Outputs:\n",
    "  - `mu`: Mean vector $ \\mu(x) $\n",
    "  - `logvar`: Log variance vector $ \\log \\sigma^2(x) $\n",
    "\n",
    "‚û°Ô∏è These two vectors define a multivariate **Gaussian posterior** $ q_\\phi(z|x) = \\mathcal{N}(\\mu, \\text{diag}(\\sigma^2)) $\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. **Reparameterization Trick**\n",
    "\n",
    "```python\n",
    "def reparameterize(self, mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "```\n",
    "\n",
    "- Draws $ z \\sim \\mathcal{N}(\\mu, \\sigma^2) $ in a way that's **differentiable**\n",
    "- Trick: $ z = \\mu + \\sigma \\cdot \\epsilon $, where $ \\epsilon \\sim \\mathcal{N}(0, I) $\n",
    "\n",
    "‚û°Ô∏è This allows **gradient-based optimization** through the sampling step.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. **Decoder**\n",
    "\n",
    "```python\n",
    "self.fc_dec = nn.Linear(latent_dim, 64 * (self.feature_map_size ** 2))\n",
    "self.decoder = nn.Sequential(\n",
    "    nn.Unflatten(1, (64, self.feature_map_size, self.feature_map_size)),\n",
    "    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=4, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(32, in_channels, kernel_size=4, stride=4, padding=0),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "- `fc_dec`: Converts latent vector $ z $ back into a low-res feature map.\n",
    "- `Unflatten`: Turns it into 3D feature map.\n",
    "- `ConvTranspose2d`: Upsamples the feature maps back to 256√ó256 image.\n",
    "- `Sigmoid`: Final output in range $[0, 1]$, appropriate for BCE loss.\n",
    "\n",
    "‚û°Ô∏è Models $ p_\\theta(x|z) $, i.e., the **reconstruction** process.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. **Forward Pass**\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    mu = self.fc_mu(encoded)\n",
    "    logvar = self.fc_logvar(encoded)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    dec_input = self.fc_dec(z)\n",
    "    x_recon = self.decoder(dec_input)\n",
    "    return x_recon, mu, logvar\n",
    "```\n",
    "\n",
    "Returns:\n",
    "- $ \\hat{x} $: reconstructed image\n",
    "- $ \\mu $, $ \\log\\sigma^2 $: for KL divergence in the loss function\n",
    "\n",
    "---\n",
    "\n",
    "## VAE Loss Function\n",
    "\n",
    "You'd typically compute this separately in training:\n",
    "\n",
    "```python\n",
    "def vae_loss(x, x_recon, mu, logvar):\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "```\n",
    "\n",
    "- **Reconstruction loss**: pixel-level difference\n",
    "- **KL divergence**: pushes posterior $ q(z|x) $ toward prior $ p(z) \\sim \\mathcal{N}(0, I) $\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Your VAE, Conceptually\n",
    "\n",
    "| Component        | PyTorch Code        | Math / Concept                        |\n",
    "|------------------|---------------------|---------------------------------------|\n",
    "| Encoder          | `Conv2d` + `Flatten`| Learns $ \\mu(x), \\log\\sigma^2(x) $    |\n",
    "| Latent space     | `fc_mu`, `fc_logvar`| Parametrize $ q_\\phi(z \\|x) $         |\n",
    "| Reparameterize   | `mu + std * eps`    | Enables backpropagation through noise |\n",
    "| Decoder          | `fc_dec` + `ConvT`  | Learns $ p_\\theta(x \\|z) $            |\n",
    "| Output           | `Sigmoid()`         | Normalized pixel intensity            |\n",
    "| Loss             | recon + KL          | ELBO objective                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad937ae7e4c674",
   "metadata": {},
   "source": [
    "Why use the covariance matrix in the notation?  \n",
    "$$\n",
    "q_\\phi(z|x) = \\mathcal{N}(\\mu, \\text{diag}(\\sigma^2))\n",
    "$$  \n",
    "means that **the approximate posterior distribution over the latent variable $ z $** is modeled as a **multivariate Gaussian** (Normal) distribution with:\n",
    "\n",
    "- Mean vector: $ \\mu = [\\mu_1, \\mu_2, ..., \\mu_d] $\n",
    "- Covariance matrix: $ \\Sigma = \\text{diag}(\\sigma^2) $\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What does `diag(œÉ¬≤)` mean?\n",
    "\n",
    "It refers to a **diagonal covariance matrix**:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_2^2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma_d^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "- The distribution has **no correlations** between different latent dimensions.\n",
    "- Each latent variable $ z_i $ is **independent** of the others.\n",
    "- But each one has its **own variance** $ \\sigma_i^2 $, allowing flexibility in how \"wide\" or \"narrow\" each latent dimension is.\n",
    "\n",
    "So we're modeling:\n",
    "$$\n",
    "z_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2) \\quad \\text{independently for } i = 1, \\dots, d\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why use a diagonal covariance?\n",
    "\n",
    "- **Computational efficiency**: Full covariance (non-diagonal) would require learning $ d \\times d $ parameters instead of just $ d $.\n",
    "- **Simpler reparameterization**: Sampling is easier and faster:  \n",
    "  $$\n",
    "  z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{with } \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "  $$\n",
    "- **Still expressive**: Even with independent latents, a powerful decoder can reconstruct complex data.\n",
    "\n",
    "---\n",
    "\n",
    "### In short:\n",
    "`diag(œÉ¬≤)` just means:\n",
    "- We're using a **multivariate Gaussian**\n",
    "- But with **uncorrelated latent variables**\n",
    "- Each latent variable has its own variance, forming a diagonal covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0e6e18459ea8a",
   "metadata": {},
   "source": [
    "Let‚Äôs unpack what happens if we **model a full covariance matrix** instead of a diagonal one in a VAE.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Current Setting: **Diagonal Covariance**\n",
    "\n",
    "In standard VAEs, we model the approximate posterior as:\n",
    "\n",
    "$$\n",
    "q_\\phi(z|x) = \\mathcal{N}(\\mu(x), \\text{diag}(\\sigma^2(x)))\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- Each latent variable $ z_i $ is **independent** of the others, conditioned on $ x $\n",
    "- The covariance matrix $ \\Sigma $ is diagonal: all off-diagonal entries are 0\n",
    "\n",
    "‚úÖ **Advantages**:\n",
    "- Efficient: only need to learn $ \\mu \\in \\mathbb{R}^d $, $ \\log\\sigma^2 \\in \\mathbb{R}^d $\n",
    "- Easy sampling:  \n",
    "  $$\n",
    "  z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{with } \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "  $$\n",
    "- KL divergence to prior $ p(z) = \\mathcal{N}(0, I) $ has a **closed form**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What if we use a **Full Covariance Matrix**?\n",
    "\n",
    "Now suppose:\n",
    "\n",
    "$$\n",
    "q_\\phi(z|x) = \\mathcal{N}(\\mu(x), \\Sigma(x))\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "\\Sigma(x) \\in \\mathbb{R}^{d \\times d} \\quad \\text{(not necessarily diagonal)}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- Latent variables $ z_i $ can now be **correlated**\n",
    "- We learn a **richer** posterior distribution\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ What are the trade-offs?\n",
    "\n",
    "#### ‚ùå 1. **Parameter Explosion**\n",
    "- A full covariance matrix has $ d(d+1)/2 $ parameters (symmetric).\n",
    "- For $ d = 32 $, that‚Äôs 528 parameters per sample instead of just 32 for the diagonal variance.\n",
    "\n",
    "#### ‚ùå 2. **Reparameterization Becomes Tricky**\n",
    "- You now need to **sample from a multivariate normal** with full covariance:\n",
    "  $$\n",
    "  z = \\mu + L \\cdot \\epsilon\n",
    "  $$\n",
    "  where:\n",
    "  - $ \\epsilon \\sim \\mathcal{N}(0, I) $\n",
    "  - $ L $ is the **Cholesky decomposition** of $ \\Sigma $  \n",
    "    such that $ \\Sigma = LL^\\top $\n",
    "\n",
    "  ‚ö†Ô∏è That decomposition has to be **differentiable** and **stable**, which is non-trivial during training.\n",
    "\n",
    "#### ‚ùå 3. **KL Divergence is Harder**\n",
    "- For a full covariance, you lose the easy closed-form KL divergence to $ \\mathcal{N}(0, I) $\n",
    "- You either:\n",
    "  - Compute it manually (more complex formula), or\n",
    "  - Use **Monte Carlo estimates** (higher variance)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What could you gain?\n",
    "\n",
    "- **More expressive latent representation**  \n",
    "  You can model more subtle relationships between latent dimensions.\n",
    "\n",
    "- **Better posterior approximation**  \n",
    "  Particularly helpful when the true posterior is **not factorized**.\n",
    "\n",
    "- **Applications**:\n",
    "  - Hierarchical VAEs\n",
    "  - Normalizing flows (extend this idea to model richer posteriors)\n",
    "  - Some types of scientific or structured data\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Summary\n",
    "\n",
    "| Aspect                    | Diagonal Covariance                        | Full Covariance                         |\n",
    "|---------------------------|--------------------------------------------|-----------------------------------------|\n",
    "| Param count               | Linear in latent dim                       | Quadratic in latent dim                 |\n",
    "| Sampling                  | Easy $( z = \\mu + \\sigma \\cdot \\epsilon $) | Needs Cholesky: $ z = \\mu + L\\epsilon $ |\n",
    "| KL Divergence             | Closed-form                                | Complicated or sampled                  |\n",
    "| Computational Cost        | Low                                        | High                                    |\n",
    "| Flexibility               | Lower (independent dims)                   | Higher (models correlations)            |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Alternative: **Low-rank + Diagonal** (Compromise)\n",
    "Some VAEs use:\n",
    "$$\n",
    "\\Sigma = D + UU^\\top\n",
    "$$\n",
    "Where:\n",
    "- $ D $: diagonal (easy to learn)\n",
    "- $ UU^\\top $: low-rank correlation structure (compact & efficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971df4a2da7a4da",
   "metadata": {},
   "source": [
    "Let‚Äôs explore `feature_map_dim` ‚Äî what it **represents**, **why it matters**, and **how it connects to the math** of VAEs.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© What is `feature_map_dim`?\n",
    "\n",
    "In your `AitexVAE` class, this line:\n",
    "\n",
    "```python\n",
    "self.feature_map_dim = 256 * (self.feature_map_size ** 2)\n",
    "```\n",
    "\n",
    "means:\n",
    "\n",
    "> The output of the final encoder layer is a **feature map** with shape:  \n",
    "> $$\n",
    "> (\\text{batch\\_size}, 256, H', W') \\quad \\text{where } H' = W' = \\text{feature\\_map\\_size}\n",
    "> $$\n",
    "> And when we `Flatten()` it, we get a 1D vector of size:  \n",
    "> $$\n",
    "> 256 \\cdot H' \\cdot W' = \\text{feature\\_map\\_dim}\n",
    "> $$\n",
    "\n",
    "This flattened vector is the **input to the latent layers** `fc_mu` and `fc_logvar`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why do we need `feature_map_dim`?\n",
    "\n",
    "We need to transform the high-dimensional feature map into a **vector** of size `latent_dim`.  \n",
    "That‚Äôs where:\n",
    "\n",
    "```python\n",
    "self.fc_mu = nn.Linear(feature_map_dim, latent_dim)\n",
    "self.fc_logvar = nn.Linear(feature_map_dim, latent_dim)\n",
    "```\n",
    "\n",
    "come in ‚Äî they **project the flattened features into a latent Gaussian distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Example\n",
    "\n",
    "If your input image is 256√ó256, and you downsample it 4√ó using stride-2 convolutions:\n",
    "\n",
    "- Feature map size $ H' = W' = 256 / 2^4 = 16 $\n",
    "- Final conv layer outputs 256 channels\n",
    "- So:\n",
    "  $$\n",
    "  \\text{feature\\_map\\_dim} = 256 \\cdot 16 \\cdot 16 = 65,\\!536\n",
    "  $$\n",
    "\n",
    "Your encoder now reduces a **256√ó256 image** into a **65,536-dimensional vector**, which is compressed into a small latent space (e.g. 32).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How does this connect to the math?\n",
    "\n",
    "In VAE theory, we define an **encoder network** $ q_\\phi(z|x) $ which produces:\n",
    "- $ \\mu(x) \\in \\mathbb{R}^d $\n",
    "- $ \\log\\sigma^2(x) \\in \\mathbb{R}^d $\n",
    "\n",
    "To compute this, we first extract features from the image:\n",
    "\n",
    "1. Image $ x \\in \\mathbb{R}^{1 \\times 256 \\times 256} $\n",
    "2. Feature extraction (convolutions) ‚Üí high-level representation\n",
    "3. Flatten into vector of shape $ \\mathbb{R}^{\\text{feature\\_map\\_dim}} $\n",
    "4. Feed into dense layers:\n",
    "   $$\n",
    "   \\mu(x) = W_\\mu \\cdot h + b_\\mu \\\\\n",
    "   \\log\\sigma^2(x) = W_{\\log\\sigma^2} \\cdot h + b_{\\log\\sigma^2}\n",
    "   $$\n",
    "   where $ h \\in \\mathbb{R}^{\\text{feature\\_map\\_dim}} $\n",
    "\n",
    "So mathematically:\n",
    "\n",
    "> `feature_map_dim` is the dimensionality of the **intermediate variable \\( h \\)** from which the Gaussian parameters of \\( q(z|x) \\) are derived.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What happens if `feature_map_dim` is too big or small?\n",
    "\n",
    "- **Too big** (e.g. not enough downsampling):\n",
    "  - High memory use\n",
    "  - Risk of overfitting\n",
    "  - Slower training\n",
    "- **Too small** (too aggressive downsampling):\n",
    "  - May lose spatial detail\n",
    "  - Bottleneck may be too tight ‚Üí poor reconstructions\n",
    "\n",
    "So it's a **crucial trade-off** between:\n",
    "- **Representational power**: Enough detail to model complex data\n",
    "- **Compression**: Low enough to enable meaningful latent representations\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Concept           | Code/Math                         | Meaning                                                        |\n",
    "|-------------------|-----------------------------------|----------------------------------------------------------------|\n",
    "| `feature_map_dim` | `C √ó H' √ó W'`                     | Dimensionality of flattened encoder output                     |\n",
    "| Used in           | `fc_mu`, `fc_logvar`              | Projects to mean & logvar of latent distribution               |\n",
    "| VAE math role     | Intermediate representation $ h $ | Input to the parameter functions of $ q(z              \\\\|x) $ |\n",
    "| Key trade-off     | Detail vs. compression            | Affects capacity of latent space & training efficiency         |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc8c2a86e6e100",
   "metadata": {},
   "source": [
    "üî• Why use sigmoid in the decoder and when to use it not?\n",
    "\n",
    "### üß™ The short answer:\n",
    "> **You don‚Äôt have to use a `Sigmoid` activation at the decoder output if you're not using BCE or normalizing the output range.**  \n",
    "> It depends on:\n",
    "> - Your **loss function**\n",
    "> - Your **data scaling**\n",
    "> - What **range** your model should predict\n",
    "\n",
    "Let‚Äôs break it down properly üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why is `Sigmoid` used at all?\n",
    "\n",
    "A `Sigmoid` squashes the output to the range $ (0, 1) $, which is useful when:\n",
    "\n",
    "- Your **input pixel values are normalized** to $[0, 1]$\n",
    "- Your **loss** expects that range, e.g., **Binary Cross-Entropy (BCE)**:\n",
    "  $$\n",
    "  \\text{BCE}(x, \\hat{x}) = -\\left[ x \\log \\hat{x} + (1 - x) \\log(1 - \\hat{x}) \\right]\n",
    "  $$\n",
    "  This loss is only valid when $ \\hat{x} \\in (0, 1) $\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What happens when you use **MSE or Frequency loss**?\n",
    "\n",
    "If you're **not using BCE**, but:\n",
    "\n",
    "- **MSE**:\n",
    "  $$\n",
    "  \\text{MSE}(x, \\hat{x}) = \\frac{1}{N} \\sum_i (x_i - \\hat{x}_i)^2\n",
    "  $$\n",
    "- **FFT/Frequency-based loss**: compares magnitude spectra in frequency domain\n",
    "\n",
    "Then:\n",
    "\n",
    "### ‚û§ `Sigmoid` is **not mathematically necessary**.\n",
    "\n",
    "However‚Ä¶\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î So, should you remove `Sigmoid`?\n",
    "\n",
    "Let‚Äôs consider a few cases:\n",
    "\n",
    "| Use Case | Input Range | Loss Function | Should Use `Sigmoid`? |\n",
    "|----------|-------------|----------------|------------------------|\n",
    "| Grayscale images ‚àà [0, 1] | [0, 1] | **BCE** | ‚úÖ Yes |\n",
    "| Grayscale ‚àà [0, 1] | [0, 1] | **MSE** | ‚ùå Optional ‚Äì can keep or remove |\n",
    "| Raw pixel ‚àà [0, 255] | [0, 255] | **MSE** | ‚ùå No ‚Äî use `ReLU` or no activation |\n",
    "| You normalize with `mean=0.5, std=0.5` | [‚àí1, 1] | **MSE or others** | ‚ùå No ‚Äî better use `Tanh` or `Linear` |\n",
    "| Frequency-domain loss (e.g. FFT) | any | spectral | ‚ùå No ‚Äî decoder output should stay unconstrained |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Specific to **your case** (AITEX + MSE + Frequency loss):\n",
    "\n",
    "- Your input is likely **grayscale**, possibly normalized to $[0, 1]$\n",
    "- You're using **MSE + spectral loss** (e.g., L2 between FFTs)\n",
    "- Your **anomaly detection task** is **not binary**, so BCE isn‚Äôt necessary\n",
    "- A `Sigmoid` would:\n",
    "  - **Restrict decoder output** to [0, 1]\n",
    "  - **Act as a soft clipping function**, potentially hurting performance on fine textures or spectra\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Recommendation\n",
    "\n",
    "If:\n",
    "- You normalize inputs to $[0, 1]$\n",
    "- You use MSE loss\n",
    "\n",
    "Then:\n",
    "‚Üí **Remove `Sigmoid`** or replace it with **`ReLU`** or even just keep **`Linear`** (no activation).\n",
    "\n",
    "So this:\n",
    "\n",
    "```python\n",
    "self.decoder = nn.Sequential(\n",
    "    ...\n",
    "    nn.ConvTranspose2d(32, in_channels, kernel_size=4, stride=4, padding=0),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "Becomes:\n",
    "\n",
    "```python\n",
    "self.decoder = nn.Sequential(\n",
    "    ...\n",
    "    nn.ConvTranspose2d(32, in_channels, kernel_size=4, stride=4, padding=0)\n",
    "    # optionally: nn.ReLU() or nn.Tanh()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Side Note: `Tanh` vs `ReLU` vs `None`\n",
    "\n",
    "| Activation | Output Range | Use when... |\n",
    "|------------|---------------|-------------|\n",
    "| `Sigmoid` | (0, 1) | BCE loss, normalized grayscale |\n",
    "| `Tanh` | (‚àí1, 1) | Inputs are normalized with mean=0, std=1 |\n",
    "| `ReLU` | [0, ‚àû) | Outputs should be non-negative but not clipped |\n",
    "| `None` | (‚àí‚àû, ‚àû) | FFT / regression / advanced losses |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
